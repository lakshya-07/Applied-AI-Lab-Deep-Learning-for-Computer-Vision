{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<p>\n",
    "  <b>AI Lab: Deep Learning for Computer Vision</b><br>\n",
    "  <b><a href=\"https://www.wqu.edu/\">WorldQuant University</a></b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <p>\n",
    "    <center><b>Usage Guidelines</b></center>\n",
    "  </p>\n",
    "  <p>\n",
    "    This file is licensed under <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International</a>.\n",
    "  </p>\n",
    "  <p>\n",
    "    You <b>can</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">✓</span> Download this file</li>\n",
    "      <li><span style=\"color: green\">✓</span> Post this file in public repositories</li>\n",
    "    </ul>\n",
    "    You <b>must always</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">✓</span> Give credit to <a href=\"https://www.wqu.edu/\">WorldQuant University</a> for the creation of this file</li>\n",
    "      <li><span style=\"color: green\">✓</span> Provide a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">link to the license</a></li>\n",
    "    </ul>\n",
    "    You <b>cannot</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: red\">✗</span> Create derivatives or adaptations of this file</li>\n",
    "      <li><span style=\"color: red\">✗</span> Use this file for commercial purposes</li>\n",
    "    </ul>\n",
    "  </p>\n",
    "  <p>\n",
    "    Failure to follow these guidelines is a violation of your terms of service and could lead to your expulsion from WorldQuant University and the revocation your certificate.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing the packages we'll need in this notebook.  They're mostly the same as the ones we've used in the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import ultralytics\n",
    "import yaml\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to reproduce this notebook in the future, we'll record the version information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Platform:\", sys.platform)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"---\")\n",
    "print(\"matplotlib version:\", plt.matplotlib.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"PIL version:\", Image.__version__)\n",
    "print(\"PyYAML version:\", yaml.__version__)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"ultralytics version:\", ultralytics.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will require a GPU to run in any reasonable amount of time.  Check that the device is `cuda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the work in this lesson will be getting all of the annotation data in the format that YOLO expects it.  Once that's done, we can relax while the computer runs many epochs of training.\n",
    "\n",
    "In a previous lesson, we arranged the Dhaka AI data set in the `data_images` directory.  Here's the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree data_images --filelimit=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.1:** Set the directories for the training images and annotations as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = pathlib.Path(\"data_images\", \"train\")\n",
    "images_dir = ...\n",
    "annotations_dir = ...\n",
    "\n",
    "print(\"Images     :\", images_dir)\n",
    "print(\"Annotations:\", annotations_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves what the annotation XML files look like.  This next cell will print out the first 25 lines of the first annotation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 25 $annotations_dir/01.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each detected object is described in an `<object>` tag.  The `<name>` tag gives us the class of the object, and the `<bndbox>` gives the upper-left and lower-right corners  of the bounding box.  This is a sensible and readable format.\n",
    "\n",
    "Unfortunately, this is not the format that YOLO needs.  The YOLO format is a text file, with each object being a line of the format\n",
    "```\n",
    "class_index x_center y_center width height\n",
    "```\n",
    "where `class_index` is a number assigned to class.  The bounding box is centered at (`x_center`, `y_center`), with a size of `width`$\\times$`height`.  All of these dimensions are given as a fraction of the image size, rather than in pixels.  These are called _normalized_ coordinates.\n",
    "\n",
    "Let's start by assigning the class indices.  We happen to know that these are the classes in this data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"ambulance\",\n",
    "    \"army vehicle\",\n",
    "    \"auto rickshaw\",\n",
    "    \"bicycle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"garbagevan\",\n",
    "    \"human hauler\",\n",
    "    \"minibus\",\n",
    "    \"minivan\",\n",
    "    \"motorbike\",\n",
    "    \"pickup\",\n",
    "    \"policecar\",\n",
    "    \"rickshaw\",\n",
    "    \"scooter\",\n",
    "    \"suv\",\n",
    "    \"taxi\",\n",
    "    \"three wheelers (CNG)\",\n",
    "    \"truck\",\n",
    "    \"van\",\n",
    "    \"wheelbarrow\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "A puzzle for you to consider: If we didn't know that these were the classes in this data set, how would you have found this list?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.2:** Generate a dictionary that maps the classes names to their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = ...\n",
    "\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work out the bounding box transformation \"by hand\" first.  We'll use the object we saw above as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1200\n",
    "height = 800\n",
    "xmin = 833\n",
    "ymin = 390\n",
    "xmax = 1087\n",
    "ymax = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.3:** Compute the center of the bounding box.  We take it to be halfway between the min and max values.  We also need to divide by the width or height as appropriate, to measure it in a fraction of the image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_center = ...\n",
    "y_center = ...\n",
    "\n",
    "print(f\"Bounding box center: ({x_center}, {y_center})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "Sanity check: Consider the $y$ coordinates.  Does this value for center make sense?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.4:** Compute the width and height of the bounding box.  Again, measure it as a fraction of the width or height of the whole image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_width = ...\n",
    "bb_height = ...\n",
    "\n",
    "print(f\"Bounding box size: {bb_width:0.3f} ⨯ {bb_height:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "Sanity check: Consider the height.  Does this value make sense?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.5:** Encapsulate this code in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_yolo_bbox(bbox, width, height):\n",
    "    \"\"\"Convert the XML bounding box coordinates into YOLO format.\n",
    "\n",
    "    Input:  bbox    The bounding box, defined as [xmin, ymin, xmax, ymax],\n",
    "                    measured in pixels.\n",
    "            width   The image width in pixels.\n",
    "            height  The image height in pixels.\n",
    "\n",
    "    Output: [x_center, y_center, bb_width, bb_height], where the bounding\n",
    "            box is centered at (x_center, y_center) and is of size\n",
    "            bb_width x bb_height.  All values are measured as a fraction\n",
    "            of the image size.\"\"\"\n",
    "\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    ...\n",
    "\n",
    "    return [x_center, y_center, bb_width, bb_height]\n",
    "\n",
    "\n",
    "xml_to_yolo_bbox([xmin, ymin, xmax, ymax], width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.6:** Write a function to parse all of the objects in an XML file.  Much of the code comes from a previous lesson.  We need to add code to look up the class index from the class name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(f):\n",
    "    \"\"\"Parse all of the objects in a given XML file to YOLO format.\n",
    "\n",
    "    Input:  f      The path of the file to parse.\n",
    "\n",
    "    Output: A list of objects in YOLO format.\n",
    "            Each object is a list [index, x_center, y_center, width, height].\"\"\"\n",
    "\n",
    "    objects = []\n",
    "\n",
    "    tree = ET.parse(f)\n",
    "    root = tree.getroot()\n",
    "    width = int(root.find(\"size\").find(\"width\").text)\n",
    "    height = int(root.find(\"size\").find(\"height\").text)\n",
    "\n",
    "    for obj in root.findall(\"object\"):\n",
    "        label = obj.find(\"name\").text\n",
    "        class_id = ...\n",
    "        bndbox = obj.find(\"bndbox\")\n",
    "        xmin = int(bndbox.find(\"xmin\").text)\n",
    "        ymin = int(bndbox.find(\"ymin\").text)\n",
    "        xmax = int(bndbox.find(\"xmax\").text)\n",
    "        ymax = int(bndbox.find(\"ymax\").text)\n",
    "        yolo_bbox = xml_to_yolo_bbox([xmin, ymin, xmax, ymax], width, height)\n",
    "\n",
    "        objects.append(...)\n",
    "\n",
    "    return objects\n",
    "\n",
    "\n",
    "objects = parse_annotations(annotations_dir / \"01.xml\")\n",
    "print(\"First object:\", objects[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.7:** Write a function that outputs the YOLO objects in the text format.  Each object should be on its own line, with spaces between the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_label(objects, filename):\n",
    "    \"\"\"Write the annotations to a file in the YOLO text format.\n",
    "\n",
    "    Input:  objects   A list of YOLO objects, each a list of numbers.\n",
    "            filename  The path to write the text file.\"\"\"\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        for obj in objects:\n",
    "            # Write the object out as space-separated values\n",
    "\n",
    "            # Write a newline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll test this on the objects we've parsed.  Check that the first line agrees with the values from the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_label(objects, \"yolo_test.txt\")\n",
    "!head -n 1 yolo_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now set up to convert all of the annotations into the correct format.  But where should we put them?  YOLO has a distinct file layout it expects, so we'll have to put both the formatted annotations and the images in new locations.\n",
    "\n",
    "First, let's look at the types of images that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.8:** Determine the types of images in the training set, from their file extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions = ...\n",
    "\n",
    "print(extensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't technically a problem&mdash;YOLO can read PNG files as well as JPEG files.  But as a format, PNG is inefficient for photographs, compared to JPEG.  Additionally, some of these files are mildly corrupted.  YOLO can open them, but it'll print out warnings when it does.\n",
    "\n",
    "To address both of these issues, we'll convert all of the images to RGB JPEG files before feeding them into YOLO.  The following function implements the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image(fin, fout):\n",
    "    \"\"\"Open the image at `fin`, convert to a RGB JPEG, and save at `fout`.\"\"\"\n",
    "    Image.open(fin).convert(\"RGB\").save(fout, \"JPEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.9:** Convert the PNG image `193.png` to a JPEG.  Then display the original and converted versions to check by eye that the image is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_image = images_dir / \"193.png\"\n",
    "convert_image(...)\n",
    "\n",
    "display.display(\n",
    "    Image.open(images_dir / \"193.png\"),\n",
    "    Image.open(...)  # Add path to the test JPEG\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, YOLO expects a directory structure like so:\n",
    "```\n",
    "data_yolo\n",
    "├── images\n",
    "│   ├── train\n",
    "│   └── val\n",
    "└── labels\n",
    "    ├── train\n",
    "    └── val\n",
    "```\n",
    "You may find this surprising.  (Your author certainly did!)  The top-level split is between the images and labels, not between the training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.10:** Create the directory structure for YOLO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_base = pathlib.Path(\"data_yolo\")\n",
    "\n",
    "# It's important to clear out the directory, if it already\n",
    "# exists.  We'll get a different train / validation split\n",
    "# each time, so we need to make sure the old images are\n",
    "# cleared out.\n",
    "shutil.rmtree(yolo_base, ignore_errors=True)\n",
    "\n",
    "(yolo_base / \"images\" / \"train\").mkdir(parents=True)\n",
    "# Create the remaining directories.\n",
    "...\n",
    "\n",
    "!tree $yolo_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the directory structure in place, the code below will iterate through all of the images in the original `images_dir`.  It will randomly assign each image either to the training or validation split.  It will parse the annotations file and write it to the correct subdirectory in `data_yolo/labels`.  And it will convert the image into a JPEG and write it to the correct subdirectory in `data_yolo/images`.\n",
    "\n",
    "Before we run this, and indeed before processing any large set of data, it's worth considering what might go wrong.  We don't want to get 95% of the way through this process and run into an error that forces us to restart.\n",
    "\n",
    "In this case, the `parse_annotations` function causes us the most worry.  There's all sorts of things that can go wrong with XML parsing, and we haven't written any checks for that yet.\n",
    "\n",
    "What should we do if we hit an error?  If there are only a few errors in a large data set, that's generally not worth the effort.  We should just skip any files we can't process.  We'll check at the end that we only got a few failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.11:** Add error handling around the `parse_annotations` function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "images = list(images_dir.glob(\"*\"))\n",
    "\n",
    "for img in tqdm(images):\n",
    "    split = \"train\" if random.random() < train_frac else \"val\"\n",
    "\n",
    "    annotation = annotations_dir / f\"{img.stem}.xml\"\n",
    "    # This might raise an error:\n",
    "    parsed = parse_annotations(annotation)\n",
    "    \n",
    "    dest = yolo_base / \"labels\" / split / f\"{img.stem}.txt\"\n",
    "    write_label(parsed, dest)\n",
    "\n",
    "    dest = yolo_base / \"images\" / split / f\"{img.stem}.jpg\"\n",
    "    convert_image(img, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only three errors in three thousand files.  That's 0.1% failure rate, so it's not worth fixing these problems.\n",
    "\n",
    "Interestingly, only one error was the XML parsing error we expected.  The other two are divide-by-zero errors caused by an image size being incorrectly noted as $0\\times0$ pixels.  Catching all exceptions turned out to be a good idea in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.12:** Check that we ended up with the expected 80/20 split between training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count = ...\n",
    "val_count = ...\n",
    "total_count = train_count + val_count\n",
    "\n",
    "print(f\"Training fraction:   {train_count/total_count:0.3f}\")\n",
    "print(f\"Validation fraction: {val_count/total_count:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "This technique of randomly assigning each item to a split won't give the exact ratio, but it should be pretty close.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for training a YOLO model needs to be described in a YAML file.  YAML is a structured document format, somewhat like JSON.  (In fact, YAML is a superset of JSON, so any JSON document is also a valid YAML file.)  Python dictionaries, lists, strings, and numbers map naturally to YAML constructions, so we'll start by defining a dictionary with the necessary keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.13:** Create a dictionary with the appropriate keys for a YOLO data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"path\": str(\n",
    "        yolo_base.absolute()\n",
    "    ),  # It's easier to specify absolute paths with YOLO.\n",
    "    \n",
    "    \"train\": ..., # Training images, relative to above.\n",
    "    \n",
    "    \"val\": ..., # Validation images\n",
    "    \n",
    "    \"names\": ..., # Class names, as a list\n",
    "    \n",
    "    \"nc\": ..., # Number of classes\n",
    "}\n",
    "\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.14:** Write the YAML file to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_config = \"data.yaml\"\n",
    "# Using yaml.safe_dump() protects you from some oddities in the YAML format.\n",
    "# It takes the same arguments as json.dump().\n",
    "yaml.safe_dump(...)\n",
    "\n",
    "\n",
    "!cat data.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous lesson, we used to `YOLOv8s` (*s* for small) model for object detection.  This time, we'll use the `YOLOv8n` (*n* for nano) model.  The nano model is less than 30% of the size of the small model, but it still manages 80% of the small model's performance.  This'll cut down the training time without hurting performance too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.15:** Load the pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model that is located at `runs/detect/train/weights/last.pt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"> <strong>Regarding Model Training Times</strong>\n",
    "\n",
    "This task involves training the model for more 30 epochs, which might take a long time and your lab might run out of resources. Instead of training it from scratch, load the pre-trained model using the cells below.\n",
    "\n",
    "<b>Loading the model instead of training it is the only way to pass the activity.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Changes with respect to the video</b></p>\n",
    "<p>The instructor in the video is using <code>results.save_dir</code> to explore the results of the train process, but as we have loaded the model instead of training, we'll need to define a new variable <code>save_dir</code>.</p>\n",
    "    <p>From now on, any reference that you see to <code>results.save_dir</code> should be replaced with <code>save_dir</code>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this\n",
    "save_dir = Path(\"runs/detect/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process, various diagnostic information is saved to disk.  This is easier to understand than all of that output above.  You can see the directory printed out at the end of the output.  (It will be `runs/detect/train` the first time you run that cell, but will change the next time.)  The directory is also available as a property of the `results` object returned by the `train` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `tree` command to display the contents of the save directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree $save_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.16:** Display and examine the precision-recall curves for the model.  They are plotted in `PR_curve.png`.  Remember that the more area under the curve, the better the model is performing.  Which classes does the model do well at detecting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.17:** Display the confusion matrix, which is already plotted for us in `confusion_matrix_normalized.png`.  Note which classes the model is most effective on.  Are they the same as those suggested by the precision-recall curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data about the training process are available in the `results.csv` file.  We can load it in with pandas, although we need a few options to make the result look nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(save_dir / \"results.csv\", skipinitialspace=True).set_index(\n",
    "    \"epoch\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.18:** Plot the classification loss over time for both the training and validation data.  Comparing these curves helps us understand if we are overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<p>Don't see the plot?  Run the code below, and then try the plotting cell again.</p>\n",
    "<p><pre><code>plt.close('all')\n",
    "plt.switch_backend('module://matplotlib_inline.backend_inline')</code>\n",
    "</pre>\n",
    "<p>This happens with YOLO experiences an error while generating its plots.  It doesn't reset the matplotlib settings correctly.  The code above sets things right again.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `.plot` method on DataFrames may be useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snapshots of the model are saved in the `weights` subdirectory.  The weights for the last training epoch as well as the best performing epoch, as measured by the validation data, are saved.  This allows us to load the trained model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.19:** Load the weights that gave the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = YOLO(...)\n",
    "\n",
    "print(saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "Each time you run the predict function loop, YOLO writes out information to <code>runs/detect/predict##</code>.  This can be useful if you want to assemble many results.  If you want to clean up all the old data, run: <code>!rm -r /runs/detect/predict*</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.20:** Conduct object detection on frame 600 that was extracted from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = saved_model.predict(\n",
    "    ..., # Path to an image\n",
    "\n",
    "    # Only return objects detected at this confidence level or higher\n",
    "    conf=0.5,\n",
    "    # Save output to disk\n",
    "    save=True,\n",
    ")\n",
    "\n",
    "f\"Results type: {type(predict_results)}, length {len(predict_results)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a list with a single element.  The bounding boxes detected are listed in the `boxes` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results[0].boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some properties worth noting:\n",
    "- `cls` lists the class indices\n",
    "- `conf` gives the confidence of there being an object\n",
    "- `xywh` lists the centers and sizes of the bounding boxes in pixels\n",
    "- `xywhn` gives the same in normalized coordinates.\n",
    "- `xyxy` and `xyxyn` give the corners of bounding boxes in pixels and normalized coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4.21:** Get the names of the classes of objects detected in this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the tensor is on the GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the bounding box information to plot these objects on the image.  But with the `save=True` option we gave the predict call, this image has already been created for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(pathlib.Path(predict_results[0].save_dir) / \"frame_600.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "This file &#169; 2024 by [WorldQuant University](https://www.wqu.edu/) is licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "096a2d255787421bb46f690694d5a705": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0a9aa8e03bc34199b0d686dd6af43a2c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_5da8ce0492404656a6d76913e4914b17",
       "max": 3003,
       "style": "IPY_MODEL_bd1fec540f8340c29ed063c2dba4229f",
       "value": 3003
      }
     },
     "19800c35b32a41028e4e091e5cc0cc80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4423c111c857481d82265db66594090b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5da8ce0492404656a6d76913e4914b17": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "631b9fa3141d4abfb4093195ce56b6ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4423c111c857481d82265db66594090b",
       "style": "IPY_MODEL_19800c35b32a41028e4e091e5cc0cc80",
       "value": " 3003/3003 [01:24&lt;00:00, 45.19it/s]"
      }
     },
     "66c0bca932cf476a8d204898aa9b7857": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a7607506b35f4a96beec30a06e4a75d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d7d2d014d07246e4ae5374dc759758ae",
       "style": "IPY_MODEL_66c0bca932cf476a8d204898aa9b7857",
       "value": "100%"
      }
     },
     "bd1fec540f8340c29ed063c2dba4229f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d5fa41a1ca3b48e89d3d2e046914c37e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a7607506b35f4a96beec30a06e4a75d3",
        "IPY_MODEL_0a9aa8e03bc34199b0d686dd6af43a2c",
        "IPY_MODEL_631b9fa3141d4abfb4093195ce56b6ae"
       ],
       "layout": "IPY_MODEL_096a2d255787421bb46f690694d5a705"
      }
     },
     "d7d2d014d07246e4ae5374dc759758ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
