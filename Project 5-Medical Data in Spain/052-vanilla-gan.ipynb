{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "  <b>AI Lab: Deep Learning for Computer Vision</b><br>\n",
    "  <b><a href=\"https://www.wqu.edu/\">WorldQuant University</a></b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <p>\n",
    "    <center><b>Usage Guidelines</b></center>\n",
    "  </p>\n",
    "  <p>\n",
    "    This file is licensed under <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International</a>.\n",
    "  </p>\n",
    "  <p>\n",
    "    You <b>can</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">✓</span> Download this file</li>\n",
    "      <li><span style=\"color: green\">✓</span> Post this file in public repositories</li>\n",
    "    </ul>\n",
    "    You <b>must always</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">✓</span> Give credit to <a href=\"https://www.wqu.edu/\">WorldQuant University</a> for the creation of this file</li>\n",
    "      <li><span style=\"color: green\">✓</span> Provide a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">link to the license</a></li>\n",
    "    </ul>\n",
    "    You <b>cannot</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: red\">✗</span> Create derivatives or adaptations of this file</li>\n",
    "      <li><span style=\"color: red\">✗</span> Use this file for commercial purposes</li>\n",
    "    </ul>\n",
    "  </p>\n",
    "  <p>\n",
    "    Failure to follow these guidelines is a violation of your terms of service and could lead to your expulsion from WorldQuant University and the revocation your certificate.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can develop networks, we'll need to import the tools we'll be using and get our data.\n",
    "\n",
    "First, let's get our imports. These are all ones we've used in previous projects, with the possible exception of `datetime`. We'll use that to get the current time and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also the print the versions of the packages we're using. If someone else looks at this, or we come back to it later, we'll know what was working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"torch version : \", torch.__version__)\n",
    "print(\"torchvision version : \", torchvision.__version__)\n",
    "print(\"numpy version : \", np.__version__)\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with networks, so we'll want to use GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with a new dataset for this project. It will be images, as in the last several projects. This time we'll be looking at medical images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <p>\n",
    "  <strong>Warning: difference with video</strong>\n",
    "        </p>\n",
    "    <p>\n",
    "  The video associated to this lesson shows the instructor downloading the dataset from GCP. We've modified this project to make that data available without the need of downloading it. You can skip the parts where the instructor downloads and extracts the compressed data.\n",
    "    </p>\n",
    "    <p>The resulting directory and data are the same.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few images to see what we're working with. We'll need a path to our files. They're in the `data_p5/gan_training_images` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.1:** Create a variables to access the `data_p5` and `data_p5/gan_training_images/` directories. Use `pathlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = ...\n",
    "images_dir = ...\n",
    "print(images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will randomly select and display a few images. We have a collections of MRI scans of brains. We're resizing them to fit them on the screen, so some may look a little squashed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = list(images_dir.glob(\"*.jpg\"))\n",
    "\n",
    "image_sample = random.sample(image_list, 8)\n",
    "\n",
    "resize = transforms.Resize((300, 300))\n",
    "gray = transforms.Grayscale()\n",
    "\n",
    "images = []\n",
    "for file in image_sample:\n",
    "    img = read_image(str(file))\n",
    "    img = gray(resize(img))\n",
    "    images.append(img)\n",
    "\n",
    "real_grid = make_grid(images, nrow=4, pad_value=255.0)\n",
    "torchvision.transforms.ToPILImage()(real_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to set these up so PyTorch can access them. This means creating an `ImageFolder`. We'll also need to transform the images as they come in. To keep computations under control, we'll resize the images to $64$ x $64$ and make them grayscale. So we'll need to make the following transformations:\n",
    "\n",
    "- Convert to grayscale\n",
    "- Resize to $64$ x $64$\n",
    "- Convert to tensor\n",
    "\n",
    "The grayscale conversion reduces us to one color channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.2:** Create the transformations to resize and convert to tensor. The grayscale is already provided. Use the `IMAGE_SIZE` variable when resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "\n",
    "transformations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms...\n",
    "    ]\n",
    ")\n",
    "\n",
    "transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the directory structure we have, we'll want to create an `ImageFolder` on the `data_dir`. This will read in all of our images into one class (corresponding to the `gan_training_images` directory). This will be fine for us, since we won't be using the class information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.3:** Create an `ImageFolder` using our transformations, and a `DataLoader`. Use a batch size of $128$ and make sure to turn on shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = ...\n",
    "\n",
    "dataset = ...\n",
    "dataloader = ...\n",
    "\n",
    "single_batch = next(iter(dataloader))[0]\n",
    "print(f\"Batches have shape: {single_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be getting $128$ images, with one color channel, of size $64$ x $64$. This gives us a set of \"good\" medical images - ones that look like the real thing. We'll need these as a reference point when we try to generate \"fake\" ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with generative adversarial networks (GANs), we generally need two networks. One is called the generator. It attempts to make new images. The second is the discriminator. It tries to tell if an image is real or created by the generator. The GAN process pits the two against each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by creating the discriminator, as it's more familiar. It's an image classifier, similar to what we've made in earlier projects. We'll make a simpler one for this, so it runs a bit faster. Instead of using a convolutional network, we'll flatten our image down to a vector and use standard `Linear` (fully connected) layers. For our activation functions, we'll use `LeakyReLU(0.25)`, except for the final layer. \n",
    "\n",
    "We're trying to tell real from fake images. We could think of this as two classes, but really it's only one: if it's not real, it's fake. So we'll produce an output with one value for each image. With only one output, we'll need the `Sigmoid` to get a result between $0$ and $1$, instead of the `SoftMax` we used with multiple classes.\n",
    "\n",
    "Our network will be:\n",
    "\n",
    "- Flatten image\n",
    "- Linear layer with 1024 neurons\n",
    "- `LeakyReLU(0.25)`\n",
    "- Linear layer with 512 neurons\n",
    "- `LeakyReLU(0.25)`\n",
    "- Linear Layer with 256 neurons\n",
    "- `LeakyReLU(0.25)`\n",
    "- Linear layer to produce one output (1 neuron)\n",
    "- `Sigmoid`\n",
    "\n",
    "We'll build this with a `Sequential` container as we have done in previous projects. Here are the first few layers to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = nn.Sequential()\n",
    "discriminator.append(nn.Flatten())\n",
    "\n",
    "# Input images are 1 * 64 * 64 = 4096 pixels after flattening\n",
    "discriminator.append(nn.Linear(1 * 64 * 64, 1024))\n",
    "discriminator.append(nn.LeakyReLU(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we should be getting an output with the shape `[batch_size, 1024]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator(single_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.4:** Add the 512 neuron linear layer, `LeakyReLU`, 256 linear layer, and `LeakyReLU` (in that order).\n",
    "\n",
    "Make sure the `negative_slope` of the `LeakyReLU` layers have a `0.25` negative slope. If you make a mistake, you might need to recreate from scratch your `discriminator` model. Execute the cell above again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add linear layer with 512 and Leaky ReLU\n",
    "# Add linear layer with 256 and Leaky ReLU\n",
    "discriminator...\n",
    "\n",
    "discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting results with following shape:\")\n",
    "print(discriminator(single_batch).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should have given us a result with shape `[batch_size, 256]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "If you got an error, remember that <tt>append</tt> modifies the network, you may need to restart it from scratch by running the previous few cells.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output will be one value for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.5:** Add a linear layer with one output (neuron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the linear layer with one neuron\n",
    "discriminator...\n",
    "\n",
    "print(\"Getting results with following shape:\")\n",
    "print(discriminator(single_batch).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be getting one result per input image now, so a `[batch_size, 1]` tensor. Without an activation function we could be getting any value for output. We want a value between $0$ and $1$. Let's put on the `Sigmoid` and see what we now have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.append(nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the discriminator. We also need a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "If you're wondering if a convolutional neural network would do a better job here, the answer is almost certainly yes. We went with the flattened image and fully connected route for speed and simplicity. We also wanted symmetry with the generator we're about to create using fully connected layers. That isn't necessary, it's completely fine to have the discriminator and generator made out of different pieces. Networks are \"black boxes\", they can't see inside each other. And you <i>can</i> make a generator using a version of convolutions, but it's more complicated.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator is a new structure for us. It will need to create new images out of nothing. How can we do this?\n",
    "\n",
    "We'll start by creating some random numbers. We don't want to create the entire image randomly, since that would just give us noise. Instead, we'll create a smaller _noise vector_, basically an array of random numbers of some size. We'll pick $100$ here, as a reasonable compromise size. \n",
    "\n",
    "We'll then run a process similar to our discriminator in reverse. Instead of getting progressively smaller numbers of neurons per layer, we'll get progressively more, until we have the $4096$ we need for a $1$ x $64$ x $64$ image. We'll reshape that into an image, and output it.\n",
    "\n",
    "We'll break our generator into three _upsampling_ stages. In each of these we'll expand the size of our vector, with a `Linear` layer with more outputs than inputs. We'll turn off the `bias` term, that's the $b$ in the linear neuron equation:\n",
    "\n",
    "$$\\mathrm{out} = \\sum_i w_i\\cdot \\mathrm{input}_i + b$$\n",
    "\n",
    "We'll then have a `BatchNorm1d`. This acts to speed up the training and make it go smoother. We'll follow that up with a `LeakyReLU(0.25)` activation function, the same as the discriminator. Here's the first upsampling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_size = 100\n",
    "first_stage_size = 256\n",
    "\n",
    "generator = nn.Sequential()\n",
    "generator.append(nn.Linear(noise_size, first_stage_size, bias=False))\n",
    "\n",
    "# The batch norm doesn't change the shape, but needs the number of inputs\n",
    "# The 0.8 adjusts its behavior, we'll use the same value for all stages\n",
    "generator.append(nn.BatchNorm1d(first_stage_size, 0.8))\n",
    "generator.append(nn.LeakyReLU(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes our $100$ random numbers and gives us $256$. Let's get a batch of random numbers to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of random numbers\n",
    "random_number_sample = torch.randn(batch_size, noise_size)\n",
    "random_number_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.6:** Run the current `generator` with just the first upsampling on the `random_number_sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_upscale = ...\n",
    "\n",
    "print(f\"After first upscale: {first_upscale.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second upsampling will look just like our first, except this time we'll go from $256$ inputs to $512$ outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.7:** Add the second upsampling stage of a linear layer, `BatchNorm1d`, and `LeakyReLU` to the `generator`. Remember we're setting `bias=False` in the `Linear` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_stage_size = ...\n",
    "\n",
    "# Add second upsampling stage\n",
    "generator...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check the output shape - it should be a `[batch_size, 512]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(random_number_sample).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a third upsampling stage to double the number of outputs again. This will take it from $512$ to $1024$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.8:** Add the third upsampling stage of a linear layer, `BatchNorm1d`, and `LeakyReLU` to the `generator`. Remember we're setting `bias=False` in the `Linear` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_stage_size = ...\n",
    "\n",
    "# Add third upsampling stage\n",
    "generator..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check the output shape - it should be a `[batch_size, 1024]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(random_number_sample).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target output size is $4096$, so we could double two more times. Instead we'll do that in a single stage, to keep the network size under control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.9:** Add the final `Linear` layer, going from the previous layer to $4096$. Remember we're setting `bias=False` in the `Linear` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the final linear layer\n",
    "generator...\n",
    "\n",
    "generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be our final stage, so we'll use a different activation function. We aren't restricted to a classification output like the discriminator was, but we need something that has image-like values. We could use the `Sigmoid` again, but instead we'll use the `Tanh` this time. \n",
    "\n",
    "We'll also need to reshape our $4096$ to an image. We'll use `Unflatten` here, the opposite of `Flatten`. It takes a single dimension and expands it to whatever shape we ask for. We'll tell it to take the second dimension of the tensor (the $4096$ rather than the batch size) and expand it to a `[1, 64, 64]`. Recall that Python is zero-based indexing, so the second dimension is called $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.append(nn.Tanh())\n",
    "generator.append(nn.Unflatten(1, [1, 64, 64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last check - we should now be getting batches of images! They should be `[batch_size, 1, 64, 64]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.10:** Run the `generator` on the `random_number_sample` and get the shape of its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_output = ...\n",
    "output_shape = ...\n",
    "\n",
    "print(f\"Generator output shape: {output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our generator!\n",
    "\n",
    "Right now it's untrained, so it will create images that are just noise. Here's what one looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(generator_output[0, 0].detach(), cmap=\"gray\")\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "You don't have to double the output size at each stage. You could do more or less, the important thing is the number increases. Doubling is common because it's convenient to work with. If you make your own, you can play with this process. You could also use a different activation function, or see what happens if the <tt>Linear</tt> layers have bias, or leave off the batch norm. You'll get different results, but they may be better than ours or worse. We make no claim that this is the best network. In fact it's not, as we'll see when we generate images.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GAN System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training will work a bit differently than in earlier projects. We aren't training a single network, but instead we're training two networks against each other. We can still use the same tools, but we'll need to organize things differently. We also don't have already labeled data, we'll have to mark our images as real or fake (we'll use the label $1$ for real and $0$ for fake)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start setting this up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this training process is more complicated, it's going to take a while. We'll want to set up some version of checkpointing. We'll keep it simple and save the model each epoch, regardless of loss values. Let's make a separate directory for each time we run this, we can label them with the current date and time. We'll use `datetime` to get the time and format it in a readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the time in the order year-month-day_hour-minute-second\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.11:** Create a path variable to the directory `runs`, and a subdirectory named using the `now` variable. Create those directories. Use `pathlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_dir = ...\n",
    "now_dir = ...\n",
    "\n",
    "# Create the directories with mkdir\n",
    "\n",
    "print(f'directory \"{now_dir}\" exists: {now_dir.exists()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "It's very likely that the time you're seeing doesn't match your clock. This is for two reasons. One is that it's looking at the time for the server, which is probably in a different time zone than you are. The other is that it may be using UTC rather than any local time zone.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the models, we'll need optimizers. The discriminator and generator will need separate ones, since they're going to have different goals. We want the discriminator to get better at telling real from fake, and the generator to get better at fooling the discriminator. So an optimization step for one makes the other one worse!\n",
    "\n",
    "Thankfully, we can use the same _kind_ of optimizer for both, with the same settings. We'll use `AdamW`, a modified version of the `Adam` that we've been using. We'll also adjust two of its settings: `lr` and `betas`. We found that modifying these gave us better training results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.12:** Fill in the missing parameters for the discriminator and generator optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "disc_opt = torch.optim.AdamW(params=..., lr=lr, betas=betas)\n",
    "gen_opt = torch.optim.AdamW(params=..., lr=lr, betas=betas)\n",
    "\n",
    "n_disc_pars = len(disc_opt.param_groups[0][\"params\"])\n",
    "n_gen_pars = len(gen_opt.param_groups[0][\"params\"])\n",
    "\n",
    "print(f\"disc_opt sees {n_disc_pars} parameters, should be 8\")\n",
    "print(f\"get_opt sees {n_gen_pars} parameters, should be 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a loss function. We'll use the same one for both, the `BCELoss`. This is the binary cross entropy, very similar to the cross entropy we've used in previous projects. The biggest differences are that this is only expecting one class, and that it expects predictions that have been run through the `Sigmoid` function. If we hadn't included the `Sigmoid` to get predictions between $0$ and $1$, we'd need the `BCEWithLogitsLoss` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our last setup step, let's make sure our models are on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.13:** Move the discriminator and the generator to `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator to device\n",
    "\n",
    "# generator to device\n",
    "\n",
    "disc_dev = next(generator.parameters()).device.type\n",
    "gen_dev = next(generator.parameters()).device.type\n",
    "\n",
    "print(f\"Discriminator on {disc_dev}\")\n",
    "print(f\"Generator on {gen_dev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to need to have the generator create images at each step in the training. It's important that these are new images, based on a new random number draw. If we used the same random number, the model would only ever be able to create one set of images! We want it to create an endless supply, that's the point. Since we'll be doing that a lot, let's make a function for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.14:** Fill in the missing parts of the function. You can use the same code we used when we were testing the generator earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_images(batch_size, generator=generator):\n",
    "    # Create a batch of random numbers\n",
    "\n",
    "    random_number_sample = random_number_sample.to(device)\n",
    "    generator.eval()\n",
    "\n",
    "    # Run the generator on the random numbers\n",
    "\n",
    "    return generator_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run\n",
    "sample_images = make_random_images(batch_size, generator)\n",
    "print(f\"Output shape: {sample_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be getting batches of images, with shape `[128, 1, 64, 64]` since we have `batch_size` of $128$. They're noise right now, but we'll want to print a sample as we train to see how the model is improving. This function displays them on screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image_tensor, n_to_display=6):\n",
    "    grid = make_grid(image_tensor[:n_to_display], nrow=6, normalize=True)\n",
    "    img_out = transforms.ToPILImage()(grid)\n",
    "    plt.figure(figsize=(15, 7.5))\n",
    "    plt.imshow(img_out)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how they look right at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(sample_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, our training will be a number of epochs. Each epoch is one pass through the real data, broken into minibatches. So our training process will be a loop over epochs, and each of those loops is a loop over batches. Let's call the processing of one batch a batch step.\n",
    "\n",
    "Each batch step in our training will follow this pattern:\n",
    "\n",
    "- Take a batch of real images, label them all $1$\n",
    "- Train the discriminator one step on the real images\n",
    "- Have the generator create a batch of fake images, label them all $0$\n",
    "- Train the discriminator one step on the fake images\n",
    "- Get the predictions of the discriminator on the fake images\n",
    "- Train the generator to trick the discriminator\n",
    "\n",
    "To break the code up a bit, let's make a function to perform one batch step. We can then loop over that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_batch_step(\n",
    "    discriminator,\n",
    "    generator,\n",
    "    real_data_batch,\n",
    "    loss_function,\n",
    "    disc_opt,\n",
    "    gen_opt,\n",
    "    device=device,\n",
    "):\n",
    "    \"\"\"Perform a single batch step\"\"\"\n",
    "\n",
    "    # Set real and fake labels\n",
    "    real_label_val = 1.0\n",
    "    fake_label_val = 0.0\n",
    "\n",
    "    # Send real data to device\n",
    "    # This pulls out just the images, we'll make our own label\n",
    "    real_images = real_data_batch[0].to(device)\n",
    "\n",
    "    # Create labels: all 1.0 for real data\n",
    "    actual_batch_size = real_images.size(0)\n",
    "    real_label = torch.full((actual_batch_size, 1), real_label_val, device=device)\n",
    "\n",
    "    # Get the derivative for the real images\n",
    "    disc_opt.zero_grad()\n",
    "    real_output = discriminator(real_images)\n",
    "    real_loss = loss_function(real_output, real_label)\n",
    "    real_loss.backward()\n",
    "\n",
    "    # Generate fake images using the generator\n",
    "    fake_images = make_random_images(actual_batch_size, generator)\n",
    "    # label all fake images as 0.0\n",
    "    fake_label = torch.full((actual_batch_size, 1), fake_label_val, device=device)\n",
    "\n",
    "    # Get the derivative for the fake images\n",
    "    fake_output = discriminator(fake_images.detach())\n",
    "    fake_loss = loss_function(fake_output, fake_label)\n",
    "    fake_loss.backward()\n",
    "\n",
    "    # Discriminator total loss\n",
    "    disc_loss = real_loss.item() + fake_loss.item()\n",
    "\n",
    "    # Train the discriminator\n",
    "    disc_opt.step()\n",
    "\n",
    "    # Get derivative for the generator\n",
    "    # We're adjusting the generator to make the\n",
    "    # discriminator think fake images are real\n",
    "    gen_opt.zero_grad()\n",
    "    trick_output = discriminator(fake_images)\n",
    "    trick_loss = loss_function(trick_output, real_label)\n",
    "    trick_loss.backward()\n",
    "\n",
    "    # Train the generator\n",
    "    gen_opt.step()\n",
    "\n",
    "    # Generator loss\n",
    "    gen_loss = trick_loss.item()\n",
    "\n",
    "    # Return discriminator loss and generator loss for logging\n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in hand, we can now look at a whole epoch. At each epoch we will:\n",
    "\n",
    "- Loop over our data loader\n",
    "- For each batch, run it through the `perform_batch_step` function\n",
    "- Checkpoint both the discriminator and the generator\n",
    "- Generate a batch of images to see the model's progress\n",
    "- Print a summary of the model's losses and display the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function we can call to checkpoint. We'll get the state dictionary of the discriminator and generator, and save them to files named `discriminator_{epoch}.pth` and `generator_{epoch}.pth`, respectively. This will give us a file for each epoch, which we can load if something goes wrong or when the training is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.15:** Fill in the missing pieces in the `save_models` function to save the discriminator and generator state dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(discriminator, generator, epoch, directory):\n",
    "    # Get discriminator state dictionary\n",
    "    disc_state_dict = ...\n",
    "\n",
    "    disc_filename = directory / f\"discriminator_{epoch}.pth\"\n",
    "\n",
    "    # Save discriminator save dictionary to `disc_filename`\n",
    "\n",
    "    # Get generator state dictionary\n",
    "    gen_state_dict = ...\n",
    "\n",
    "    gen_filename = directory / f\"generator_{epoch}.pth\"\n",
    "\n",
    "    # Save generator save dictionary to `gen_filename`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call it to save the state before we start training. This isn't necessary, since it's just random right now, but it will help us make sure it works. We'll give it an epoch number of `untrained` to make it clear it's not part of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(discriminator, generator, \"untrained\", now_dir)\n",
    "\n",
    "print(f\"Files in {now_dir}\")\n",
    "for filename in now_dir.glob(\"*\"):\n",
    "    print(\"\\t\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Over Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to put things together for an epoch of training. We'll train on our real image data loader, checkpoint our model, and print the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.16:** Fill in the missing checkpoint and creation of fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    discriminator,\n",
    "    generator,\n",
    "    real_image_loader,\n",
    "    loss_function,\n",
    "    disc_opt,\n",
    "    gen_opt,\n",
    "    epoch,\n",
    "    device=device,\n",
    "):\n",
    "    # train the model\n",
    "    total_disc_loss = 0\n",
    "    total_gen_loss = 0\n",
    "    for real_data_batch in tqdm(real_image_loader):\n",
    "        disc_loss, gen_loss = perform_batch_step(\n",
    "            discriminator,\n",
    "            generator,\n",
    "            real_data_batch,\n",
    "            loss_function,\n",
    "            disc_opt,\n",
    "            gen_opt,\n",
    "            device,\n",
    "        )\n",
    "        # Keep a running total of losses from each batch\n",
    "        total_disc_loss += disc_loss\n",
    "        total_gen_loss += gen_loss\n",
    "\n",
    "    # Save the models at the current epoch\n",
    "\n",
    "    print(f\"Epoch {epoch} finished\")\n",
    "    print(f\"Discriminator loss: {disc_loss}, Generator loss: {gen_loss}\")\n",
    "\n",
    "    # Create 6 images\n",
    "    sample_images = ...\n",
    "\n",
    "    # Display the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it for one epoch. If things worked, we'll get a bar filling up, then a loss and some sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch(\n",
    "    discriminator, generator, dataloader, loss_function, disc_opt, gen_opt, epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can loop over this to train for many epochs. We'll train for two more to demonstrate, and you'll see the model start to improve a bit. Even after one epoch it's no longer just noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 3\n",
    "\n",
    "for epoch in range(1, total_epochs):\n",
    "    train_epoch(\n",
    "        discriminator,\n",
    "        generator,\n",
    "        dataloader,\n",
    "        loss_function,\n",
    "        disc_opt,\n",
    "        gen_opt,\n",
    "        epoch=epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to train this quite a bit longer before it starts generating somewhat reasonable images. We trained a model for $100$ epochs, which took about an hour, and saved the weights. You're welcome to try to train it yourself by increasing the number of epochs and continuing the training. But you can also load our weights using the next cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <p>\n",
    "  <strong>Warning: difference with video</strong>\n",
    "        </p>\n",
    "    <p>\n",
    "  The video associated to this lesson shows the instructor downloading a pretrained model from GCP. We have since changed the project to make this data already available in the notebook. You can proceed normally, the files <code>discriminator_99.pth</code> and <code>generator_99.pth</code>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load discriminator weights\n",
    "disc_weights = torch.load(\"discriminator_99.pth\")\n",
    "discriminator.load_state_dict(disc_weights)\n",
    "\n",
    "# Load generator weights\n",
    "gen_weights = torch.load(\"generator_99.pth\")\n",
    "generator.load_state_dict(gen_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "If you train the system for longer, you'll see the losses aren't decreasing. Even though the images are getting better, the losses are going up and down. This is completely expected. As the generator gets better at fooling the discriminator, the generator's loss improves and the discriminator's gets worse. But as the discriminator gets better at detecting fakes, the discriminator's loss improves and the generator's gets worse. This back-and-forth is what makes the GAN work, but it means the loss itself isn't very helpful.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you trained it yourself or used our weights, let's see how it did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.2.17:** Create 6 more images with our now-trained model and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 6 images\n",
    "sample_images = ...\n",
    "\n",
    "# Display the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the real data. Here we're looking at it as the model actually saw it: scaled down to $64$ x $64$ images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_batch = next(iter(dataloader))[0]\n",
    "display_images(single_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model didn't do very well. We're getting things that are starting to have the right shape, round parts with white boundaries (the skull). Unfortunately, this isn't a situation where more training will help. The problem here is that our model is just too small and simple to produce good results. But we chose a smaller model to show how things work. A larger, more complex model would take much longer to set up and train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This file &#169; 2024 by [WorldQuant University](https://www.wqu.edu/) is licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
