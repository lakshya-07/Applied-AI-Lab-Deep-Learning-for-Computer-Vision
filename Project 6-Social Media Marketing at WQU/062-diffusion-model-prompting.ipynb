{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import diffusers\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import transformers\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll print out version numbers of the critical packages, to help with future reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Platform:\", sys.platform)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"---\")\n",
    "print(\"diffusers version:\", diffusers.__version__)\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"PIL version:\", Image.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if a GPU is available.  If not, this notebook will take a long time to run!  If a GPU is available, we'll use the \"half-precision\" `float16` data type.  This speeds up calculations, but isn't available on standard CPUs.  On those, we'll use the standard precision `float32` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(f\"Using {device} device with {dtype} data type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Diffusion attempts to produce an image aligned to a text description.  We're going to start by working on that text description.  We need to convert the human-readable text into an *embedding*.  This means that the text will be represented by a bunch of numbers, which the model will be able to ingest.\n",
    "\n",
    "Stable diffusion uses text embeddings from the Contrastive Language-Image Pre-Training (CLIP) model.  We need two pieces from this model.  First, the tokenizer splits up a string into tokens, a word, part of a word, or single character.  We load only this component by specifying the `subfolder` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.CLIPTokenizer.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\",\n",
    "    torch_dtype=dtype\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.1:** Call `tokenizer` with the text `\"Hello, world!\"` as an argument. Examine the return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world!\"\n",
    "result = ...\n",
    "\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This displays a common behavior of the transformers and diffusers libraries: they return custom containers with their results.  We can access elements of these containers ...\n",
    "\n",
    "... indexing by key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or with an attribute name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `input_ids` attribute contains the list of tokens that the string was split into.  These tokens are represented by ID numbers.  You can find the string for a given token ID number with the `tokenizer.decode` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.2:** Call `tokenizer.decode` on each of the token IDs in `result.input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in result.input_ids:\n",
    "    print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in addition to words, there are tokens representing punctuation and the start and end of the text.\n",
    "\n",
    "For our first image from Stable Diffusion, we're going to use the following prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A red bird flies through a blue sky over a green tree.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize it, but we'll do two things differently:\n",
    "- We'll fix the number of output tokens.\n",
    "- We'll have it return a PyTorch tensor instead of a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",  # Give us as many tokens as the model can handle.\n",
    "    truncation=True,  # Truncate the output if it would give us more tokens.\n",
    "    return_tensors=\"pt\",  # Return a PyTorch tensor.\n",
    ")\n",
    "\n",
    "print(text_tokens.input_ids)\n",
    "print(text_tokens.input_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Diffusion works with a model *conditioned* on our text.  For this to work, we need an *unconditioned* result to compare to.  For this, we'll use the empty string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.3:** Create tokens for the empty string input.  Assign these to `uncond_token`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncond_tokens = ...\n",
    "\n",
    "print(uncond_tokens.input_ids)\n",
    "print(uncond_tokens.input_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tokens will be converted into a text embedding with the `text_encoder` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = transformers.CLIPTextModel.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\",\n",
    "    torch_dtype=dtype\n",
    ")\n",
    "embedder.to(device)  # Do this on the GPU\n",
    "\n",
    "# Print out a summary of this neural network\n",
    "summary(embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call it with the token IDs to generate the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # No need for gradient calculations\n",
    "    text_embedding = embedder(text_tokens.input_ids.to(device))\n",
    "\n",
    "print(type(text_embedding))\n",
    "print(text_embedding.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding that we want is the `last_hidden_state` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.4:** Determine the class and shape of the `last_hidden_state` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class:\", ...)\n",
    "print(\"Shape:\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, each token has been mapped into a 768-dimension vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.5:** Compute the embedding for the unconditioned tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    uncond_embedding = ...\n",
    "\n",
    "print(uncond_embedding.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.6:** Concatenate the unconditioned embedding followed by the text embedding into a tensor called `all_embeddings`.  (This will be convenient for a later step.)  You'll find `torch.cat` useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = ...\n",
    "\n",
    "print(all_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these text embeddings are ready, we're going to set them aside for a bit.  Don't worry&mdash;we'll come back to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Random Latent Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Diffusion uses a *Variational Auto-Encoder* (*VAE*) to generate images from *latent vectors*.  At this point, we'll just create some random latent vectors.  We'll improve them in a later stage.\n",
    "\n",
    "Let's load in the VAE model. Again, we specify this through the `subfolder` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = diffusers.AutoencoderKL.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\",\n",
    "    torch_dtype=dtype\n",
    ")\n",
    "vae.to(device)  # Run it on the GPU\n",
    "\n",
    "summary(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This VAE is constructed mainly of convolutional layers, and it's able to produce outputs of various sizes.  We'll aim to produce 512$\\times$512 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 512\n",
    "width = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder will downscale both the width and height by a factor of 8.  The decoder will upscale by the same factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of channels in the latent vector is stored in the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = vae.config.latent_channels\n",
    "print(n_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent vectors will be a 4-D tensor, representing (batch, channel, height, width).  Don't be confused by the fact that there happen to be four channels.  That is a complete coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.7:** Fill in the correct shape for a latent vector below.  It should have a batch size of 1 and appropriate dimensions to produce a 512$\\times$512 image. The call to `torch.randn` will generate a random latent vector of that size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_shape = (\n",
    "    ..., # Batch size\n",
    "    ..., # Latent channels\n",
    "    ..., # Height\n",
    "    ..., # Width\n",
    ")\n",
    "\n",
    "random_latents = torch.randn(latent_shape, device=device, dtype=dtype)\n",
    "\n",
    "print(random_latents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are four channels, we can't visualize the latent vectors as an RGB image.  This function will plot each channel separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latents(latents):\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        plt.imshow(latents[0, i].cpu().numpy())\n",
    "        plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.8:** Plot the random latent vector.  If it's random, it should resemble static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the latent vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will use the VAE's decoder to convert the latent vectors into an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = random_latents\n",
    "# Decode the latents\n",
    "with torch.no_grad():\n",
    "    scaling_factor = torch.tensor(vae.config.scaling_factor, device=device, dtype=dtype)\n",
    "    decoded = vae.decode(latents / scaling_factor).sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.9:** Check the shape of the `decoded` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape:\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is in (batch, channels, height, width) format.  However, PIL expects images in (height, width, channels) format.  We can use the `.permute()` method to switch things around.  We'll also convert the tensor into a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permute the dimensions and convert to NumPy\n",
    "unscaled = decoded.permute(0, 2, 3, 1).cpu().numpy()\n",
    "print(\"Shape:\", unscaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.10:** Plot a histogram of the values in this array.  This will help us understand the distribution of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    ...,\n",
    "    bins=50,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that most of the values are in the range [-1, 1].  We'll scale this to the range [0, 255].  Values that start outside of this range will be clipped to the min or max.  We'll also convert the datatype to the unsigned 8-bit integer type commonly used for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the image values to be between 0 and 255\n",
    "scaled = ((unscaled + 1) * 255 / 2).clip(0, 255).round().astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.11:** Create a PIL image and display it.  You'll find `Image.fromarray()` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PIL image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look like anything in particular, because we started with a random latent vector.  But it also does not look like noise, because the latent space encodes common patterns.  We're seeing a random superposition of these common patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.12:** Create a function that returns the image corresponding to a latent vector.  We'll use this as we evolve the latent vectors later.  You should be able to complete this function by copying code from the previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latents_to_image(latents, vae=vae):\n",
    "    \"\"\"Transform the latent vector to a image, using a VAE decoder.\n",
    "\n",
    "    Inputs:  latents  Latent vector(s) as a 4-D PyTorch tensor.  Only\n",
    "                      the first element of the batch will be used.\n",
    "             vae      The VAE used to decode the image from latents.\n",
    "\n",
    "    Outputs: A PIL image corresponding to the latents.\n",
    "    \"\"\"\n",
    "    # Scaling factor\n",
    "    scaling_factor = torch.tensor(vae.config.scaling_factor, device=device, dtype=dtype)\n",
    "    \n",
    "    # Decode the latents\n",
    "    ...\n",
    "    # Permute the dimensions and convert to NumPy\n",
    "    ...\n",
    "    # Scale the image values to be between 0 and 255\n",
    "    ...\n",
    "    # Return a PIL image\n",
    "    return ...\n",
    "\n",
    "latents_to_image(random_latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image above looks nothing like our prompt, but that's to be expected.  We created it from a random latent vector.  Our next task is to perform denoising diffusion on our latent vector.\n",
    "\n",
    "To do this, we need two more components.  First, we need a model to predict the noise in our latent vector.  For that, we use a *U-Net* model.  These are similar to the ResNet models commonly used for image analysis.  This particular model is a *condition* model.  This mean that it can take our text embedding as input, to judge how well the denoised image matches the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = diffusers.UNet2DConditionModel.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "unet.to(device)\n",
    "\n",
    "summary(unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we'll see, this process works best when done in a number of small steps. (This is because the U-Net model is trained in a series of steps that progressively add more noise.)  The *scheduler* will govern this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = diffusers.UniPCMultistepScheduler.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n",
    ")\n",
    "\n",
    "print(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should start by scaling our random latent vectors by the scheduler's `init_noise_sigma` factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.13:** Create a starting latent vector by multiplying `random_latents` by the scheduler's `init_noise_sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = ...\n",
    "\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "If you happened to check, you notice that <code>scheduler.init_noise_sigma</code> is actually 1.0.  The scaling didn't actually do anything in this case.  But other schedulers may have different values here, so we'll keep this step.\n",
    "</div>\n",
    "\n",
    "For our first demonstration we'll only do a single denoising step.  Setting this causes the scheduler to compute a tensor of time steps for denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(1)\n",
    "print(\"Steps:\", scheduler.timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.14:** Get the value of the first (and only) time step in the variable `t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ...\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to run the denoising model twice, once with the unconditioned input and once with the conditioned text input.  We can be more efficient by doing them together in a batch of two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.15:** Create a `latent_inputs` tensor with two copies of `latents`.  It should be 2$\\times$4$\\times$64$\\times$64.  You'll probably find `torch.cat` helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the latent inputs\n",
    "latent_inputs = ...\n",
    "\n",
    "print(latent_inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will make a prediction of the noise.  Since `all_embeddings` contains both the unconditioned and conditioned text embeddings (in that order), we'll get noise estimates for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the noise\n",
    "scaled_inputs = scheduler.scale_model_input(latent_inputs, timestep=t)\n",
    "with torch.no_grad():\n",
    "    noise = unet(scaled_inputs, t, encoder_hidden_states=all_embeddings)\n",
    "\n",
    "# Split the unconditioned and conditioned predictions\n",
    "noise_uncond, noise_cond = noise.sample.chunk(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine these two estimates together.  The `guidance_scale` tells us how closely we should follow the prompt's conditioning.  Small values increase creativity, at the cost of not following the prompt too closely. Large values keep the image closer to the prompt, but increase the risk of artifacts.  Typical values run from 2 (lots of freedom) to 15 (follows prompt carefully)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 7.5  # A pretty middle-of-the-road value\n",
    "noise_pred = noise_uncond + guidance_scale * (noise_cond - noise_uncond)\n",
    "\n",
    "print(noise_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.16:** Plot the predicted noise.  It's the same shape as the latent vector, so you can use the `plot_latents` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the noise prediction with `plot_latents`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can remove this predicted noise from the latent vector with the scheduler's `.step` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the variable `latents` based on the predicted noise\n",
    "latents = scheduler.step(noise_pred, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.17:** Plot the denoised latent vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the latent vector with `plot_latents`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.18:** Convert the latent vector to an image and display it.  Remember the `latents_to_image` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the latent vector to an image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "Since we're starting with a random latent vector, we'll get a different image each time we run this notebook.\n",
    "</div>\n",
    "\n",
    "If you squint, you can sort of see a red bird, green tree, and blue sky.  We'll get much better results if we do more denoising steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.2.19:** Set up a loop to run the denoising process over 50 steps.  We've provided code that will display the image at every step.  It will look like noise at the beginning, and tends to coalesce only at the very end.  You should be able to complete this code by copying sections of the previous code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 50  # A typical number.  Increase for cleaner results.\n",
    "scheduler.set_timesteps(num_steps)\n",
    "\n",
    "# We'll start with the same random latent values as before.\n",
    "latents = random_latents * scheduler.init_noise_sigma\n",
    "\n",
    "# This will let us update the image through the training.\n",
    "display_handle = display(latents_to_image(latents, vae), display_id=\"latent_display\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in tqdm(scheduler.timesteps):\n",
    "        # Assemble the latent inputs\n",
    "        ...\n",
    "        # Predict the noise\n",
    "        ...\n",
    "        # Split the unconditioned and conditioned predictions\n",
    "        ...\n",
    "        # Combine the predictions according to the guidance scale\n",
    "        ...\n",
    "        # Update the variable `latents` based on the predicted noise\n",
    "        latents = ...\n",
    "        # This will update the image based on the current latent vector.\n",
    "        display_handle.update(latents_to_image(latents, vae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've made it! You've successfully prompted the diffusion model to generate an image.\n",
    "\n",
    "Next, we'll see how to use Pipelines to automate the whole process. You'll need to switch to the next notebook named `062-diffusion-pipelines.ipynb`.\n",
    "\n",
    "But before doing so! Please STOP this notebook, so you free up the GPU VRAM allocated. In other case, you won't be able to run the Pipeline (there won't be enough VRAM available).\n",
    "\n",
    "To stop this notebook, click on the Stop symbol at the top:\n",
    "\n",
    "![](stop-notebook.png)\n",
    "\n",
    "or execute the next cell that has `exit()` in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
